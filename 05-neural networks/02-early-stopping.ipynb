{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A real NN, on data and overfitting\n",
    "We'll see what overfitting means.  On a trivial dataset, which we generate, we will have the neural network fit noise.  Early stopping can solve this problem.  In this exercise, we use Pytorch with Lightning to set up your neural network.\n",
    "\n",
    "(c) Patrick van der Smagt, March 2023.  Please do not distribute this without Patrick's consent (he will give it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data set\n",
    "We create a simple artificial data set which is easy to plot.  It's a sine which is sampled, to which Gaussian noise is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #data in the test and train set\n",
    "n_train = 30\n",
    "n_val = 30\n",
    "n_test = 30\n",
    "# variance of the noise we put on top\n",
    "var = 0.2\n",
    "\n",
    "np.random.seed(0) # everyone should have the same start\n",
    "x_train = np.arange(n_train)/(n_train/10.)\n",
    "y_train = np.vectorize(math.sin)(x_train)+np.random.normal(0,var,x_train.shape[0])\n",
    "\n",
    "x_val = np.arange(n_val)/(n_val/10.) + 5./n_val\n",
    "y_val = np.vectorize(math.sin)(x_val)+np.random.normal(0,var,x_val.shape[0])\n",
    "\n",
    "x_test = np.arange(n_test)/(n_test/10.) + 5./n_test\n",
    "y_test = np.vectorize(math.sin)(x_test)+np.random.normal(0,var,x_test.shape[0])\n",
    "\n",
    "# the following may be necessary on some Macs\n",
    "x_train = x_train.astype('float32')/10.\n",
    "y_train = y_train.astype('float32')\n",
    "x_val = x_val.astype('float32')/10.\n",
    "y_val = y_val.astype('float32')\n",
    "x_test = x_test.astype('float32')/10.\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot\n",
    "Blue crosses are the validation data, green dots the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train,y_train,'go')\n",
    "plt.plot(x_val,y_val,'bx')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network\n",
    "Set up the neural network and the pytorch-lightning helper functions. \n",
    "The neural network is set up by subsequent layers, first summing, then putting the nonlinear transformation function.   It also includes a layer called \"dropout\".  That will have parameter value 0.0 for now, meaning it's not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the LightningModule\n",
    "class NeuralNetwork(pl.LightningModule):\n",
    "    def __init__(self, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout # store in the module\n",
    "        self.neuralnet =  nn.Sequential(nn.Linear(1, 50), nn.Tanh(), nn.Dropout(dropout), nn.Linear(50, 50), nn.Tanh(),nn.Linear(50, 1))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # flatten the tensor, because the Linear layer only accepts a vector (1d array)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.neuralnet(x)[:,0]\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        z = self.forward(x)\n",
    "        loss = nn.functional.mse_loss(z, y)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        z = self.forward(x)\n",
    "        loss = nn.functional.mse_loss(y, z)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        z = self.forward(x)\n",
    "        loss = nn.functional.mse_loss(y, z)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters())\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "nnn = NeuralNetwork(dropout = 0.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions for lightning that give the data to the NN training functions are defined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = (x_train, y_train), batch_size=10)\n",
    "val_loader = DataLoader(dataset = (x_val, y_val), batch_size=10)\n",
    "test_loader = DataLoader(dataset = (x_test, y_test), batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the neural network and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"tensorlogs\", name=\"sine\")\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoint\", save_top_k=1, monitor=\"val_loss\")\n",
    "trainer = pl.Trainer(max_epochs=1000, callbacks=[TQDMProgressBar(refresh_rate=0), checkpoint_callback], logger=logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(nnn, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a bit more\n",
    "trainer.fit_loop.max_epochs = 8000\n",
    "trainer.fit(nnn, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.test(ckpt_path=\"best\", dataloaders=train_loader)\n",
    "#trainer.test(ckpt_path=\"best\", dataloaders=val_loader)\n",
    "#trainer.test(ckpt_path=\"last\", dataloaders=val_loader)\n",
    "#trainer.test(ckpt_path=\"last\", dataloaders=test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training is done, we plot the result.  We do so by creating a new input set, called the test set, we run it through the NN, and plot the result.  We also plot the training and the test data, as dots and crosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(nn):\n",
    "    # plot the data, test data, and fit\n",
    "    n = 200\n",
    "    x_test = np.arange(n)/(n/10.)/10.\n",
    "    y_test = nn(torch.from_numpy(x_test).float()).data.numpy()\n",
    "    plt.plot(x_test, y_test)\n",
    "    plt.plot(x_train,y_train,'go')\n",
    "    plt.plot(x_val,y_val,'bx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(nnn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the best model w.r.t. the validation data set.  This is called \"early stopping\", as we use the model where we stopped training when the error on the validation set was lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xval_model = NeuralNetwork.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "plot_result(best_xval_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now redo the above and set `dropout=0.5`.  Compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnnd = NeuralNetwork(dropout = 0.5)\n",
    "trainer.fit_loop.max_epochs = 9000\n",
    "trainer.fit(nnnd, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "plot_result(nnnd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
