{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network learning\n",
    "===\n",
    "(c) Patrick van der Smagt, July 2018.  Contains adapted code from non-attributed sources.  Sorry for that. \n",
    "Please do not distribute this without Patrick's consent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off with defining transfer functions.  These are the (nonlinear) functions $\\phi$ in the neurons.\n",
    "\n",
    "The implementation is a bit weird: it returns $y = \\phi(x, \\mathrm{False})$ or $s = \\phi(y, \\mathrm{True})$ where $y = \\phi(x)$.  So if the second parameter is True, the first argument is interpreted to be $\\phi(x)$ and is used to compute the derivative.  This is done for computational efficiency only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function.  returns y=sigmoid(x) or y' = sigmoid(y, True)\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "# tanh function.\n",
    "def tanh(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return 1-x*x\n",
    "    return np.tanh(x)\n",
    "# tanh function.\n",
    "def linear(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return 1\n",
    "    return x\n",
    "def rectifier(x, deriv=False):\n",
    "    if (deriv==True):\n",
    "        return (x > 0.)\n",
    "    return x * (x > 0.)\n",
    "\n",
    "actfunc = tanh              # transfer function\n",
    "y = actfunc(np.arange(-4,4,.1))\n",
    "plt.plot(y, \"green\");\n",
    "s = actfunc(y,True)      # derivative\n",
    "plt.plot(s, \"blue\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with a simple neural network with no hidden layer\n",
    "---\n",
    "This simple \"neural network\"---in this form also known as perceptron---does a simple forward pass, then computes the output errors, and uses those to adapt the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dataset.  Why is the third input always 1?\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "z = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# seed random numbers\n",
    "np.random.seed()\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "# What happens if I start with w0 = 0?\n",
    "w0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "loss = []\n",
    "\n",
    "# the transfer function\n",
    "transfer = tanh\n",
    "\n",
    "# the learning rate\n",
    "learn_rate = 0.1\n",
    "\n",
    "# make so many steps\n",
    "for iter in range(10000):\n",
    "    # forward propagation: layer 0 is the input\n",
    "    l0 = X\n",
    "    # layer one is dot product of l0 with w0, \n",
    "    # then the transfer function.  l1 is the output, i.e., y\n",
    "    l1 = transfer(np.dot(l0,w0))\n",
    "\n",
    "    # What is the residual?  Note that the error\n",
    "    # equals the absolute residual.\n",
    "    residual = z - l1\n",
    "    # In the MLE interpretation we assume Gaussian errors in\n",
    "    # the data.  So the loss is the sum of squared errors:\n",
    "    loss.append(sum(i**2 for i in residual))\n",
    "\n",
    "    # To find the residual at the inputs of the output unit, we\n",
    "    # \"propagate it through the unit\", i.e., we multiply the\n",
    "    # residual with the derivative of the transfer function.\n",
    "    # We call the result \"delta\".  You can easily show that\n",
    "    # this is mathematically correct.\n",
    "    l1_delta = residual * transfer(l1,True)\n",
    "\n",
    "    # Update weights.  How can this be extended with a momentum term?\n",
    "    w0 += learn_rate * np.dot(l0.T,l1_delta)\n",
    "\n",
    "print(\"perceptron output:\")\n",
    "print(l1)\n",
    "plt.plot(np.log(loss));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Implementation\n",
    "----\n",
    "A neural network with one hidden layer.  Biases are not added by this implementation.  Also, momentum is not implemented yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP:\n",
    "    def __init__(self, n_inpt, n_hidden, n_output, hid_transfer = tanh, out_transfer = tanh):\n",
    "        self.w0 = 2*np.random.random((n_inpt, n_hidden))\n",
    "        self.w1 = 2*np.random.random((n_hidden, n_output))\n",
    "        self.hid_transfer = hid_transfer\n",
    "        self.out_transfer = out_transfer\n",
    "        self.loss = []\n",
    "    \n",
    "    def ffnn_forward(self, X):\n",
    "        l0 = X\n",
    "        l1 = self.hid_transfer(np.dot(l0,self.w0))\n",
    "        l2 = self.out_transfer(np.dot(l1,self.w1))\n",
    "        return (l0,l1,l2)\n",
    "\n",
    "    def NeuralNetwork(self, X, y, learn_rate = 0.1, momentum = 0., iter=60000):\n",
    "        # search direction for the weights in layers 1 and 0\n",
    "        optim_step1 = 0\n",
    "        optim_step0 = 0\n",
    "\n",
    "        for j in range(iter):\n",
    "\n",
    "            # Feed forward through layers 0, 1, and 2\n",
    "            # l0 is the input layer\n",
    "            # l1 is the hidden layer\n",
    "            # l2 is the output layer\n",
    "            (l0,l1,l2) = self.ffnn_forward(X)\n",
    "\n",
    "            # (signed) error at the output\n",
    "            residual = y - l2\n",
    "            # the loss is the sum of squared errors\n",
    "            self.loss.append(sum(i**2 for i in residual))\n",
    "\n",
    "            # compute the delta.\n",
    "            l2_delta = residual*self.out_transfer(l2,deriv=True)\n",
    "\n",
    "            # back-propagate the output delta to the hidden units\n",
    "            l1_error = l2_delta.dot(self.w1.T)\n",
    "\n",
    "            # compute the delta at the hidden units\n",
    "            l1_delta = l1_error * self.hid_transfer(l1,deriv=True)\n",
    "\n",
    "            optim_step1 = learn_rate*l1.T.dot(l2_delta)\n",
    "            optim_step0 = learn_rate*l0.T.dot(l1_delta)\n",
    "\n",
    "            self.w1 += optim_step1\n",
    "            self.w0 += optim_step0\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(np.log(self.loss));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the contour plot of this NN with a two-dimensional input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nncontour (start, end, step, mlp):\n",
    "    xx, yy = np.meshgrid( np.arange(start, end, step),  np.arange(start, end, step))\n",
    "    x = np.array([[0,0,1]])\n",
    "    z = np.zeros(( len(xx[:,0]), len(yy[:,0])) )\n",
    "    for i in range(len(xx[:,0])):\n",
    "        for j in range(len(yy[:,0])):\n",
    "            x = [[xx[i,j], yy[i,j], 1]]\n",
    "            (l0,l1,l2) = mlp.ffnn_forward(x)\n",
    "            z[i, j] = l2.item(0)\n",
    "    plt.figure()\n",
    "    plt.contour(xx, yy, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do a typical silly example: the XOR function.  Please play around with the parameters of the NN: number of hiddens, the learning rate, the momentum..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])\n",
    "\n",
    "mlp_xor = MyMLP(X.shape[1], 10, y.shape[1])\n",
    "mlp_xor.NeuralNetwork(X,y, 0.1, 0., 50000)\n",
    "(l0, l1, l2) = mlp_xor.ffnn_forward(X)\n",
    "print(\"output:\")\n",
    "print(l2)\n",
    "nncontour(0,1, 0.01, mlp_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Hinton diagrams\n",
    "---\n",
    "Hinton diagrams can be used to represent weights in a NN.  Each box represents a single weight.  The colour of the box indicates the sign of the weight (black: negative; white: positive); the size of the box the size of the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    if not max_weight:\n",
    "        #max_weight = 0.5 * 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n",
    "        max_weight = np.max(np.abs(matrix))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w)) / max_weight\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinton(mlp_xor.w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to fit a sine wave.  Experiment with, e.g., very many and very few hidden units.  Try it with and without momentum in the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(-3,3,0.1) # is it enough data??\n",
    "X = np.array([[x, 1] for x in data])\n",
    "y = np.array([[np.sin(x)+np.random.randn()/10 for x in data]]).T\n",
    "mlp_sin = MyMLP(X.shape[1],3,y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_sin.NeuralNetwork(X,y,0.1,0.9) # note that momentum is not implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output of the neural network over the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(l0,l1,l2) = mlp_sin.ffnn_forward(X)\n",
    "plt.plot(l2, \"g.\")\n",
    "plt.plot(y, \"b.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
