{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "Information theory is a branch of mathematics, statistics and computer science which tackles measuring the amount of information in an observation. The unit of measurement in this case is bits; or in many cases \"nats\", if the natural logarithm is used instead of the binary one.\n",
    "\n",
    "\n",
    "### What does probability theory have in common with compression?\n",
    "\n",
    "It turns out that compression algorithms induce a probability distribution and vice versa. The general idea is: if a symbol to compress appears often, we should use fewer bits to compress it than if it rarely appears. The frequency with which a symbol occurs can be estimated with statistical methods.\n",
    "\n",
    "It turns out that to encode an observation $x$, we will need $\\log p(x)$ nats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "In information theory, the entropy of a random variable is the average level of “information”, “surprise”, or “uncertainty” inherent to the variable’s possible outcomes. It is usually used to measure the uncertainty of uncertain random variables. For a discrete variable $X$ with $K$ states we define it as \n",
    "\n",
    "$$H(X) = \\sum_x p(X=x) \\log p(X=x).$$\n",
    "\n",
    "This is the average number of nats required to encode an observation of $X$.\n",
    "\n",
    "The entropy is maximised in the case of a uniform distribution. In that case, uncertainty is maximal because everything is equally likely. Entropy is minimal if $p(X=c) = 1$ for some $c$. In that case, there is no surprise because we know exactly what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_in_nats(outcomes):\n",
    "    \"\"\"Given an array of outcome probabilities, return the entropy of the random variable.\"\"\"\n",
    "    return -(outcomes * np.log(outcomes)).sum()\n",
    "def entropy_in_bits(p):\n",
    "  # check if p is a valid probability distribution\n",
    "  if np.sum(p) != 1 or np.any(p < 0) or np.any(p > 1):\n",
    "    raise ValueError(\"p must be a valid probability distribution\")\n",
    "  # compute the entropy in bits\n",
    "  return -np.sum(p * np.log2(p + 1e-9)) # add a small constant to avoid log(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy in nats is 0.6931471805599453\n",
      "entropy in bits is 0.99999999711461\n"
     ]
    }
   ],
   "source": [
    "rate = .5\n",
    "print (\"entropy in nats is\", entropy_in_nats(np.array([rate, 1 - rate])))\n",
    "print (\"entropy in bits is\", entropy_in_bits(np.array([rate, 1 - rate])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL-divergence\n",
    "\n",
    "The **Kullback-Leibler-divergence** or **relative entropy** is a method to compare two distributions. It measure the average amount of excess-nats needed to encode something generated from source $q$, given that we will encode it using distribution $p$:\n",
    "\n",
    "$$ \\mathrm{KL}(q||p) = \\sum_x q(x) \\log \\frac{q(x)}{p(x)}.$$\n",
    "\n",
    "It basically tells us how much worse we do when we want to encode $x$ if we use $p$ instead of some optimal distribution $q$.\n",
    "\n",
    "The amount of nats needed is just the **cross entropy**:\n",
    "\n",
    "$$H(q, p) = -\\sum_x q(x) \\log p(x).$$\n",
    "\n",
    "One can show:\n",
    "\n",
    "$$\\mathrm{KL}(q, p) = \\sum_x q(x) \\log q(x)  - \\sum_x q(x) \\log p(x) = -H(q) + H(q, p).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact:** The KL-divergence is always positive. \n",
    "\n",
    "Proof: See Kevin Murphy, Machine learning: a probabilistic perspective, p. 58."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020410997260127572\n"
     ]
    }
   ],
   "source": [
    "def kl(outcomes1, outcomes2):\n",
    "    return (outcomes1 * (np.log(outcomes1) - np.log(outcomes2))).sum()\n",
    "\n",
    "rate1, rate2 = .5, .6\n",
    "print(kl(np.array([rate1, 1 - rate1]), np.array([rate2, 1 - rate2])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "We are interested in how much knowing random variable $X$ tells us about random variable $Y$.\n",
    "\n",
    "We need this when we want to measure the statistical dependence between different features in data. Mutual information can be used for various purposes, such as:\n",
    "\n",
    "- Feature selection: we can select the features that have high mutual information with the target variable, and discard the features that are irrelevant or redundant.\n",
    "- Clustering: we can use mutual information to measure the similarity between clusters or between data points and clusters.\n",
    "- Representation learning: we can use mutual information to learn low-dimensional representations of data that preserve the most relevant information.\n",
    "- Model comparison: we can use mutual information to compare the performance of different models or estimators by measuring how much information they capture about the data.\n",
    "  \n",
    "Mutual information is a general and powerful tool that can capture non-linear dependencies that correlation or covariance cannot. However, it also requires estimating the probability distributions involved, which can be challenging and inaccurate for real-world data.\n",
    "\n",
    "It is not a good idea to measure mutual information with the covariance because covariance is a measure of linear dependence, whereas mutual information measures general dependence (including non-linear relations). Therefore, mutual information detects dependencies that do not only depend on the covariance. Thus, mutual information is zero when the two random variables are strictly independent, but covariance can be zero even when the two random variables are stochastically dependent.\n",
    "\n",
    "A better way is to compare the joint and the factored distributions $ p(X, Y) $ and $p(X)p(Y)$. Remember: if they were identical, $X$ and $Y$ would be identical. \n",
    "\n",
    "The KL-divergence between the two is called the **mutual information**:\n",
    "$$\n",
    " \\mathrm{MI}(X, Y) = \\mathrm{KL}(p(X, Y)\\,||\\,p(X)p(Y)).\n",
    "$$\n",
    "which is the same as \n",
    "$$\n",
    " \\mathrm{MI}(X, Y) =H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)\n",
    "$$\n",
    "The mutual information is 0 if and only if X and Y are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6906003745943332\n"
     ]
    }
   ],
   "source": [
    "def mi(outcomes1, outcomes2):\n",
    "    implementme\n",
    "    \n",
    "rate1, rate2 = .5, .6\n",
    "print (mi(np.array([rate1, 1 - rate1]), np.array([rate2, 1 - rate2])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
