{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "numpy is Python's most important package for linear algebra.  It defines vectors, matrices, and their operations, in a very efficient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important data type in numpy is ``ndarray``.  These are general matrices.  A special subclass is ``matrix``, which creates an intuitive notation for 2D matrices.  It's otherwise not very helpful and generally not used.\n",
    "\n",
    "A vector is, of course, nothing but a special type of matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "print (a, a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is not really a vector, of course, but rather the transpose of it.  We can, of course, transpose a, but it makes no real difference for the internal representation of ``a``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.transpose(a)\n",
    "print (a, a.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the nice thing of numpy is that it is under-the-hood implemented in C.  That makes it fast.  Much faster.  Compare the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare speeds\n",
    "array = np.random.rand(100)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product of a with itself is the Euclidean length squared of the vector that is represented by ``a``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "s = 0\n",
    "for i in range(0,array.shape[0]):\n",
    "    s += array[i]*array[i] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as the L2 norm (squared).  But numpy has an efficient implementation, which is not only easier to write but also faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "s = np.dot(array, array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same works for matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print (A, A.shape)\n",
    "B = np.array([[-1, 1, 2], [1, 2, -2]])\n",
    "print (B, B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we multiply two matrices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formult(M1, M2): \n",
    "    if (M1.shape[1] != M2.shape[0]):\n",
    "        raise IndexError('matrices don\\'t match for multiplication')\n",
    "    C = np.zeros((M1.shape[0], M2.shape[1]))\n",
    "    for i in range(0,M1.shape[0]):\n",
    "        for j in range(0,M2.shape[1]):\n",
    "            for k in range(0,M1.shape[1]):\n",
    "                C[i,j] += M1[i,k] * M2[k,j]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "formult(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but for loops are not efficient in python.  There usually is a simpler way to do it.  Of course, numpy has this buit in:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it is much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "for i in range(0,10000):\n",
    "    C1 = formult(A, B)\n",
    "t2 = time.time()\n",
    "for i in range(0,10000):\n",
    "    C2 = np.dot(A, B)\n",
    "t3 = time.time()\n",
    "print(C1, t2-t1)\n",
    "print(C2, t3-t2)\n",
    "print(\"the for loop is\", (t2-t1)/(t3-t2), \"times slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The identity matrix is quickly built in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.identity(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other special matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.zeros((3,4)))\n",
    "print(np.ones((2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware of how we multiply matrices with matrices (and with vectors etc.)  Which of these will work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(A, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer product is the \"opposite\" of the inner product.  While the inner product of vectors $a$ and $b$ is $a \\cdot b = a^Tb$, the outer product $a \\otimes b = a b^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.outer(A, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer product of two vectors is a matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.outer(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I can compute the dot (\"inner\") product of matrix A and B, since A has dimensions $n\\times m$ and B $m\\times p$.  But $A.A = A^T A$ does not work!  For that I have to switch rows and columns of A, called the transpose.\n",
    "The transpose of a matrix is quickly found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A, A.shape)\n",
    "print(np.transpose(A), np.transpose(A).shape)\n",
    "print(np.dot(A,A))    # goes horribly wrong, why?\n",
    "print(np.dot(A,A.T))  # A.T is a more readable way of getting transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinant\n",
    "-----------\n",
    "The determinant of a matrix is the volume of the transformation of that matrix.  For the matrix [[a,b] [c,d]]:\n",
    "<img src=\"det-2d.png\" width=\"300\">\n",
    "For a $3\\times3$ matrix:\n",
    "<img src=\"det-3d.png\" width=\"300\">\n",
    "Please compute the matrix of a $1\\times1$ matrix, of a $2\\times2$ matrix, and..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.ones((1,1)) * 10\n",
    "print(C)\n",
    "np.linalg.det(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.empty((2,2))\n",
    "C = [[1, 2], [3, 4]]\n",
    "print(np.linalg.det(C))  # 1 * 3 - 2 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.ones((2,3))\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matrix properties\n",
    "A symmetric matrix is one which equals its transpose.  Or you can say, $B_{ij} = B_{ji}$.  How do we create a symmetric matrix in Python?\n",
    "\n",
    "Let's first create a random matrix.  And its transpose.  Then (assignment) create a symmetric one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.random.randint(-10,10,size=(2,2))\n",
    "print(B)\n",
    "print(B.T)\n",
    "# now create a symmetric matrix out of B and B.T\n",
    "S = np.matmul(B.T, B) # or B.T.dot(B)\n",
    "S = B.T.dot(B)\n",
    "print(S - S.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _positive definite_ matrix, defined as a square matrix with $x^T A x > 0$ for all $x$, has all its eigenvalues positive.  Also, the diagonal elements are positive.  But... what are eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalues and -vectors\n",
    "-------------\n",
    "One can imagine eigenvectors to represent the \"invariant\" directions that a matrix represents. Eigenvectors are only defined for square matrices.  An $n\\times n$ matrix can be seen as transforming vectors from one $n$-dimensional space to another.  \n",
    "\n",
    "If $v$ is an eigenvector, $\\lambda$ is the corresponding eigenvalue such that $A v = \\lambda v$.\n",
    "\n",
    "In the below figure the blue arrow represents an eigenvector (which does not change direction) of the matrix that \"skews\" the picture.  What is the eigenvalue of the blue eigenvector?\n",
    "\n",
    "<img src=\"eigen.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([[1,2],[2,1]])\n",
    "np.linalg.eig(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition of a matrix is the largest divided by the smallest ev.\n",
    "If this number is very large (e.g. $> 10^5$ or so), the matrix is said to be badly conditioned.  It \"magnifies\" one dimension much, much more than another one!  If it is infinity, the matrix is singular: it has dependent rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.array([[1,2,3],[1,2,3],[2,4,2],[-5,3,5]])\n",
    "print(np.linalg.cond(E))\n",
    "E = np.array([[1,2],[0,0.00000001]])\n",
    "print(np.linalg.cond(E))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace of a matrix is the sum of its eigenvalues.  The determinant of a matrix is the product of its eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a set of equations\n",
    "-------------------------\n",
    "Suppose we want to solve a set of linear equations $X w = z$.  This is linear regression: how do we find the best values of the parameters $w$ such that, when multiplied with the data $X$, the desired output $z$ is given?\n",
    "\n",
    "In the mathematically clean situation, we have as many equations (rows in $X$) as parameters (rows in $w$).  In that case, $X$ is a square matrix, and the solution is found by $w = X^{-1} z$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random((2,2))\n",
    "print(X, np.linalg.cond(X))\n",
    "print(np.dot(X, np.linalg.inv(X))) #X . X^{-1} should be the indentity matrix\n",
    "w = np.dot(np.linalg.inv(X), np.random.random((2,1)))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if $X$ gets large... inversion is no longer a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random((2000,2000)) # don't run with 20000 x 20000, it is 400MB numbers!\n",
    "print(np.dot(X, np.linalg.inv(X))) #X . X^{-1} should be the indentity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this is also a problem if (a) $X$ is singular or nearly singular; and (b) when $X$ is not square.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = [[1,2.],[1,2.000000000000001]]  # note I forgot np.array().  Why does it work?\n",
    "print(np.linalg.inv(E))\n",
    "print(np.dot(np.linalg.inv(E) , E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random matrix\n",
    "X = np.random.random((5,5))\n",
    "# let's say it is singular\n",
    "X[2,:] = 0\n",
    "# compute SVD\n",
    "U,s,V = np.linalg.svd(X)\n",
    "inverse = np.dot(np.dot(V.T, np.linalg.pinv(np.diag(s))), U.T)\n",
    "print(np.dot(inverse, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Moore-Penrose pseudoinverse in numpy does exactly the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(np.linalg.pinv(X), X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it also works for non-square matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random((3,4))\n",
    "print(np.dot(np.linalg.pinv(X), X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cholesky decomposition\n",
    "\n",
    "Often, we want to find the root of a matrix: given a matrix $C$ we want to find a lower triangular matrix $L$ with $L L^T = C$. For this, the Cholesky decomposition can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random((2, 2))\n",
    "C = np.dot(X.T, X)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.linalg.cholesky(C)\n",
    "print(L)\n",
    "print(np.dot(L, L.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can only be used for matrices which are positive definite, i.e. $\\forall x: x^TCx > 0$. This is similar to the fact that we can only find the square root of positive numbers. In fact, it is a generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# reading material\n",
    "http://cs229.stanford.edu/section/cs229-linalg.pdf\n",
    "\n",
    "http://matrizen-rechner.de/ \n",
    "\n",
    "Gene Golub and Charles van Loan, Matrix Computations, John Hopkins University Press"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
