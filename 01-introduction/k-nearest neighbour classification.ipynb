{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-nearest neighbour classification\n",
    "\n",
    "This notebook contains a naive implementation of the k-nearest-neighbour (kNN) algorithm.  It is demonstrated with the IRIS data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn is a library for machine learning, which contains many standard algorithms and data sets.  We use it for loading the data set, saving writing our own parsing algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sklearn.datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning we always have inputs $x_i$ which we want to map on targets $z_i$.  We save both data sets in numpy arrays ``AX`` and ``AZ``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AX, AZ = ds.data, ds.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``.shape`` attribute of a numpy array gives us its size and form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('shape of input data:', AX.shape)\n",
    "print ('shape of target data:', AZ.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we are dealing with 150 samples of data.  Each input has 4 different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we don't just \"store\" the data, we simulate \"future data\" by reserving a part of the data set for this.  scikit-learn provides us with the necessary functionality.\n",
    "\n",
    "``StratifiedShuffleSplit`` mixes the data and separates them in a training set and a test set.  \"Stratified\" means that the number of samples per class remains constant.  If, for instance, we have a data set with 70% dogs and 30% cats, then also the training and validation sets have the approximate same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=model_selection.StratifiedShuffleSplit(n_splits=1,random_state=12)\n",
    "train_idxs, test_idxs = list(splitter.split(AX,AZ))[0]\n",
    "print(\"indices of the data for making the model:\")\n",
    "print(train_idxs)\n",
    "print()\n",
    "print(\"indices of the data for testing:\")\n",
    "print(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also give the slice operator a list of indices ``idxs``.\n",
    "# Then a new array will be created which contains exactly the corresponding\n",
    "# elements to which ``idxs`` points.\n",
    "\n",
    "X, Z = AX[train_idxs], AZ[train_idxs]\n",
    "TX, TZ = AX[test_idxs], AZ[test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = np.unique(Z).size\n",
    "print('number of classes:', n_classes)\n",
    "print('numer of data points per class:', np.bincount(Z))\n",
    "print('average input values:',  X.mean(0))\n",
    "print('variances of the inputs', X.std(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot matrix\n",
    "\n",
    "A scatterplot is useful to show high-dimensional data.  The horizontal axies shows histograms of the corresponding dimension.  Nondiagonal elements will be filled with scatterplots of the corresponding two dimensions.\n",
    "\n",
    "Colouring is done following the respective classes.  This makes it immediately clear how easy or difficult the problem is.\n",
    "\n",
    "Of course this approach is limited if information is divided amongst several dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(n_classes, n_classes, squeeze=False, figsize=(12, 12))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        if i == j:\n",
    "            _ = axs[i, j].hist(X[:, i])\n",
    "        else:\n",
    "            axs[i, j].scatter(X[:, i], X[:, j], c=Z, lw=1, s=100, alpha=.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the nearest neighbour\n",
    "\n",
    "We start by computing the k nearest neighbours for some point ``query``.\n",
    "\n",
    "For reasons of efficiency we don't use the points themselves, but their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbour_idxs(data, query, k):\n",
    "    \"\"\"Gebe die ``k`` Indizes von ``data`` zurueck, welche am naechsten\n",
    "    an ``query`` liegen.\"\"\"\n",
    "    # Wir berechnen die quadratische Distanz von jedem Punkt in ``data``\n",
    "    # zu ``query``.\n",
    "    # Auch wenn wir Euklidische Distanz annehmen, koennen wir auf das \n",
    "    # Ziehen der Wurzel verzichten, da es die Ordnung nicht veraendert.\n",
    "    # data hat die shape (N, F) wo N die Anzahl der Samples in der Datenbank\n",
    "    # ist und  F die Anzahl der Features; query hat Form (1, F)\n",
    "    difference = data - query\n",
    "    elem_sqrd_distance = difference ** 2\n",
    "    sqrd_distance = elem_sqrd_distance.sum(1)\n",
    "        \n",
    "    # Wir sortieren das gesamte Array der Distanzen. Hier geht sicherlich\n",
    "    # Performanz verloren da wir nur die kleinsten k benoetigen--aber eine\n",
    "    # effiziente Implementation wuerde den Rahmen sprengen.\n",
    "    sorted_idxs = np.argsort(sqrd_distance)\n",
    "    \n",
    "    # Hier benutzen wir den slice operator um nur die ersten k Elemente \n",
    "    # zurueckzugeben.\n",
    "    k_idxs = sorted_idxs[:k]\n",
    "    \n",
    "    return k_idxs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "Next we make a function ``predict`` which determines a class according to the neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, targets, query, k):\n",
    "    idxs = neighbour_idxs(data, query, k)\n",
    "    \n",
    "    # We use the slice operator here again.\n",
    "    neighbour_targets = targets[idxs]\n",
    "    \n",
    "    # An dieser Stelle muessen wir die naechsten Nachbarn \"abstimmen\" lassen,\n",
    "    # welche Klasse wir zurueckgeben. Beliebt ist die \"majority vote\", d.h. die\n",
    "    # Klasse welche am meisten in den Nachbarn vorkommt ist die Ausgabe. Dazu\n",
    "    # zaehlen wir die Klassen durch.\n",
    "    # Die Funktion ``bincount`` von numpy kommt da gerade recht. Sie zaehlt die \n",
    "    # Anzahl vorkommen von jedem Integer der Reihe nach durch. \n",
    "    votes = np.bincount(neighbour_targets)\n",
    "    return np.argmax(votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a best k\n",
    "\n",
    "The correct choice of k is key for the final performance.  We call this a \"hyperparameter\".  How can we determine it here without using the test set again?\n",
    "\n",
    "Idea: we repeat the split approach.  For each data point we use some candidates for k, if we take all other data into account.  This is called \"leave-one-out cross-validation\" or LOO or LOOCV.\n",
    "\n",
    "Implemented in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k(data, targets, candidates):\n",
    "    \"\"\"return that k which gives best kNN results according to LOO on the training data.\"\"\"\n",
    "    # We prepare an array filled with zeroes, counting the number of correct classifications.\n",
    "    correct = np.zeros(len(candidates))\n",
    "    \n",
    "    loo = model_selection.LeaveOneOut()\n",
    "    # Now iterate over each LOO split.\n",
    "    for train_idxs, test_idxs in loo.split(data):\n",
    "        # Here we iterate over all candidates for k.\n",
    "        for j, cand in enumerate(candidates):\n",
    "            prediction = predict(X[train_idxs], Z[train_idxs], X[test_idxs[0]], cand)\n",
    "            if prediction == Z[test_idxs[0]]:\n",
    "                # Increment if the prediction was correct.\n",
    "                correct[j] += 1\n",
    "    \n",
    "    # Return that k for which we had a maximum number of correct predictions.\n",
    "    return candidates[correct.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = find_k(X, Z, range(1, 10))\n",
    "print(\"Best k:\", best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([predict(X, Z, TX[i], best_k) for i in range(TX.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', (predictions == TZ).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depict decision boundaries\n",
    "\n",
    "We can look at the decision boundaries in which we plot the prediction of each\n",
    "point in a raster.  This only really works well in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is partly taken from  \n",
    "# http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#example-neighbors-plot-classification-py\n",
    "\n",
    "h = .1\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "zz = np.array([predict(X[:, :2], Z, np.array([x, y]), best_k) \n",
    "               for x, y in zip(xx.ravel(), yy.ravel())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, squeeze=False, figsize=(9, 9))\n",
    "ax = axs[0, 0]\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.pcolormesh(xx, yy, zz.reshape(xx.shape), alpha=.9)\n",
    "ax.scatter(TX[:, 0], TX[:, 1], c=TZ, s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. alternative distance function\n",
    "\n",
    "In ``neigbour_idxs`` we assumed Euclidean distance:\n",
    "\n",
    "$$\n",
    "d_{\\text{Euklid}}(x_i, x_j) = \\sqrt{\\sum_k (x_i^k - x_j^k)^2}\n",
    "$$\n",
    "\n",
    "An alternative would be the Manhattan distance:\n",
    "\n",
    "$$\n",
    "d_{\\text{Manhattan}}(x_i, x_j) = \\sum_k |x_i^k - x_j^k|\n",
    "$$\n",
    "\n",
    "**Exercise:** Implement the Manhattan distance in ``neighbour_idxs``. How does this effect the result?\n",
    "\n",
    "\n",
    "## 2. Skewed data\n",
    "\n",
    "Let's assume that the data is not scaled the same along each axis.  Please change the data in such a way that you multiply the first axis by 10, 100, or 1000.\n",
    "\n",
    "**Exercise:** how does this affect the distance function?  How can you deal with this problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
